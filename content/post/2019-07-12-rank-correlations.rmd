---
draft: true
title: multidimensional scaling and rank correlations
author: ~
date: 2019-06-12
slug: rank-correlations
categories: [methodology]
tags: [correlation,rank,eigendecomposition,biplot]
---

```{r knitr options, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  fig.height = 6
)
```

A fundamental idea in biplot methodology is the _conference of inertia_, a phrase i picked up from [an SO answer by ttnphns](https://stats.stackexchange.com/a/141755/68743) and quickly [incorporated into ordr](https://github.com/corybrunson/ordr/blob/master/R/ord-conference.r). The basic idea arises from the central properties of a biplot, illustrated here for principal components analysis: A case--variable data matrix $X\in\mathbb{R}^{n\times m}$ of ratio variables[^ratio] is singular-value decomposed as $X=UDV^\top$, for example the `mtcars` data set:

```{r mtcars pca}
x <- scale(mtcars, center = TRUE, scale = TRUE)
s <- svd(x)
r <- length(s$d)
```

Under this convention, $U\in\mathbb{R}^{n\times r}$ and $V\in\mathbb{R}^{m\times r}$ arise from eigendecompositions of $XX^\top$ and of $X\top X$, respectively, and $D\in\mathbb{R}^{r\times r}$ is the diagonal matrix of the square roots of their (common) eigenvalues. The matrix factors may be biplotted in three conventional ways:

* with _principal_ case coordinates $UD$ and _standardized_ variable coordinates $V$;
* with standardized case coordinates $U$ and principal variable coordinates $VD$;
* with _symmetric_ case and variable coordinates $UD^{1/2}$ and $VD^{1/2}$.

Because both sets of eigenvectors $U=\left[\,u_1\,\cdots\,u_r\,\right]$ and $V=\left[\,v_1\,\cdots\,v_r\,\right]$ are orthonormal, $U^\top U=I_r=V^\top V$ and the total inertia (variance) in each matrix is $\sum_{j=1}^{r}{{v_j}^2}=r=\sum_{j=1}^{r}{{v_j}^2}$. Meanwhile, $D$ contains all of the inertia of $X$:

[^ratio]: That is to say, if the variables don't have meaningful zero values and/or commensurate scales, then they should probably be centered to zero mean and/or scaled to unit variance.

```{r inertia}
# inertia of the (scaled) data
sum(x^2)
# inertia of the case and variable factors
sum(s$u^2)
sum(s$v^2)
# inertia of the diagonal factor
sum(s$d^2)
```

This inertia can then be _conferred_ unto the standardized case or variable coordinates, transforming one or the other into principal coordinates (the first two options above) or both halfway there (the symmetric option). Each of these options confers the inertia in such a way that the sums of the exponents of $D$ in the transformed sets of case ($F=UD^p$) and variable ($G=VD^q$) coordinates is $p+q=1$, which ensures the _inner product relationship_ $FG^\top=X$ between them. This recovers any entry $x_{ij}$ of $X$ as the inner product $f_i\cdot g_i$ of its case and variable coordinates $f_i=[\,f_{i,1}\,\cdots\,f_{i,r}\,]$ and $g_i=[\,g_{i,1}\,\cdots\,g_{i,r}\,]$.

By conferring the inertia entirely to the cases or variables, we preserve (or best approximate) the geometric configurations of the cases or variables, respectively. In PCA, the geometry of the cases is usually thought of in terms of the distances (dissimilarities) between them. Here their pairwise distances in the first two PCA dimensions are plotted against their "true" distances in the variable space:

```{r case geometry, fig.width = 5.5}
# distances between cases
x.dist <- dist(x)
# distances between cases (principal coordinates)
s.dist <- dist(s$u[, 1:2] %*% diag(s$d[1:2]))
# scatterplot
plot(
  x = as.vector(x.dist), y = as.vector(s.dist),
  asp = 1, pch = 19, cex = .5,
  xlab = "Case distances along variable coordinates",
  ylab = "Case distances in two principal coordinates"
)
```

Meanwhile, the geometry of the variables is usually understood through their covariances or correlations. The covariance between two variables $i,j$ is proportional to their inner product $$\textstyle c_{ij}=\frac{1}{n}v_i\cdot v_j=\frac{1}{n}\lVert v_i\rVert\lVert v_j\rVert\cos\theta_{ij}\text,$$ so that the cosine the angle $\theta_{ij}$ between them equals their correlation:
$$\cos\theta_{ij}=\frac{c_{ij}}{\sqrt{c_{ii}c_{jj}}/n}=\frac{c_{ij}}{\sigma_i\sigma_j}=r_{ij}$$
Here the cosines between the variable vectors in the first two PCA dimensions are plotted against their correlations across the original cases:

```{r variable geometry, fig.width = 5.5}
# correlations between variables
x.cor <- cor(x)
# magnitudes of variable vectors
s.len <- apply(s$v[, 1:2] %*% diag(s$d[1:2]), 1, norm, "2")
# cosines between variables (principal coordinates)
s.cor <- (s$v[, 1:2] / s.len) %*% diag(s$d[1:2]^2) %*% t(s$v[, 1:2] / s.len)
# scatterplot
plot(
  x = as.vector(x.cor[lower.tri(x.cor)]),
  y = as.vector(s.cor[lower.tri(s.cor)]),
  asp = 1, pch = 19, cex = .5,
  xlab = "Variable correlations among cases",
  ylab = "Cosines between variable vectors in two principal coordinates"
)
warning("COVARIANCES")
```

### multidimensional scaling of variables

The faithful approximation of inter-case distances by principal coordinates is the idea behind [(classical)](https://en.wikipedia.org/wiki/Multidimensional_scaling) _multidimensional scaling_, which can be applied to a data set of distances $\delta_{ij},\ 1\leq i,j\leq n$ in the absence of coordinates. This technique is based on the eigendecomposition of a doubly-centered matrix of squared distances, which produces matrix $UD^{1/2}$ whose first $r$ coordinates---for any $r\leq n$---recover a best approximation of the inter-case Euclidean distances in terms of the sum of squared errors, i.e. the variance of $(UD^{1/2})(UD^{1/2})^\top-\Delta=UDU^\top-\Delta$, where $\Delta=(\delta_{ij})\in\mathbb{R}^{n\times n}$. In practice, the goal is usually to position points representing the $n$ cases in a 2-dimensional scatterplot so that their Euclidean distances $\sqrt{(x_j-x_i)^2+(y_j-y_i)^2}$ approximate their original distances $\delta_{ij}$, as in this example using road distances to approximate geographic positions:

```{r multidimensional scaling, fig.width = 8}
d <- as.matrix(UScitiesD)
cent <- diag(1, nrow(d)) - matrix(1/nrow(d), nrow(d), nrow(d))
d.cent <- -.5 * cent %*% (d^2) %*% cent
d.mds <- svd(d.cent)
d.coord <- d.mds$u[, 1:2] %*% diag(sqrt(d.mds$d[1:2]))
plot(d.coord, pch = NA, asp = 1)
text(d.coord, labels = rownames(d))
```

The faithful approximation of inter-variable covariances by the inner products of their principal-coordinate vectors suggests a complementary technique that i haven't found explicitly discussed in my own background reading. Suppose we have data that is again coordinate-free but that consists this time not of distances between the cases but of covariances $c_{ij},\ 1\leq i,j\leq m$ between the variables. Had the original data been a _centered_ case--variable matrix $X$, then the covariances would been obtained as $C=\frac{1}{n}X^\top X$, which is (up to scalar) the matrix whose eigenvectors would be given by $V$ in the SVD $X=UDV^\top$. Therefore, we can fabricate coordinates for the $m$ variables that approximate what we know of their geometry---in this case, thinking of the variables as vectors, their magnitudes and the angles between them---via an eigendecomposition $C=V\Lambda V^\top$: Take $Y=V\Lambda^{1/2}\in\mathbb{R}^{m\times r}$, so that $Y^\top Y=C$. We can also obtain coordinates in fewer dimensions that best recover the geometry using the first eigenvectors and -values.

I'll validate this line of reasoning by taking the `mtcars` data set for a spin:

```{r mtcars example}
# covariances and standard deviations
c <- cov(mtcars)
s <- diag(sqrt(diag(c)))
# centered data
x <- as.matrix(scale(mtcars, center = TRUE, scale = FALSE))
# eigendecomposition of covariance matrix
c.eigen <- eigen(c)
# artificial coordinates
c.coord <- c.eigen$vectors %*% diag(sqrt(c.eigen$values))
# validate covariance recovery (up to sign)
all.equal(
  as.vector(c.coord %*% t(c.coord)),
  as.vector(c),
  tolerance = 1e-12
)
```

The use case for this technique is a situation in which covariance data exist without variable values. This may of course be the case because original data has become unavailable; but a more interesting setting that gives rise to this situation is the analysis of multiple rankings of the same set of objects in terms of their _concordance_. Rankings' concordance is often measured using rank correlations such as Kendall's tau, which may be _general correlation coefficients_ in [the sense proposed by Kendall](https://en.wikipedia.org/wiki/Rank_correlation#General_correlation_coefficient) but are not associated with an underlying (Euclidean) geometry. Nevertheless, we can use multidimensional scaling to represent these rankings as unit vectors in Euclidean space whose pairwise cosines approximate their rank correlations!

### rankings of universities

A real-world example is provided by the [Quacquarelli Symonds Top University Rankings](https://www.topuniversities.com/qs-world-university-rankings/methodology), which include rankings of hundreds of world universities along six dimensions: academic reputation, employer reputation, faculty--student ratio, citations per faculty, international faculty ratio, and international student ratio. QS weight these rankings differently in their overall assessment, but our analysis will compare the rankings to each other, so these weights are ignored. I restricted the data from the year 2020 to universities in the United States for which integer rankings (i.e. not "400+" placeholders) were available in all four years:[^unis]

[^unis]: This leaves us with only 38 universities!

```{r QS top university rankings}
qswurus20 <- readRDS(here::here("sandbox/qswurus20.rds"))
head(qswurus20)
```

Since the integer rankings were taken from the full data set, they are not contiguous (i.e. some integers between rankings never appear). To resolve this, i'll recalibrate the rankings by matching each vector of ranks to the vector of its sorted unique values:

```{r calibrate rankings}
library(dplyr)
qswurus20 %>%
  group_by(year) %>%
  mutate_at(
    vars(starts_with("rk_")),
    ~ match(., sort(unique(as.numeric(.))))
  ) %>%
  ungroup() %>%
  print() -> qswurus20
```

This subset of universities is now ranked in a consistent way along the six dimensions described above. The Kendall correlation $\kappa_{ij}$ between two rankings measures their concordance. To calculate it, every pair of universities contributes either $+1$ or $-1$ according as the rankings $i$ and $j$ agree or disagree for that pair, and the sum is scaled down by the number of pairs ${n\choose 2}$ so that the result lies between $-1$ and $1$. We interpret $\kappa_{ij}=1$ as perfect concordance, $\kappa_{ij}=-1$ as perfect discordance (the rankings reverse each other), and $\kappa_{ij}=0$ as independence (the rankings are unrelated to each other).

Since the QS rankings are not variations on a common theme, like different traditional measures of quality, i don't have any strong expectation of how concordant they'll be. A common way to visualize correlation matrices is the heatmap, so i'll use that technique first:

```{r Kendall rank correlations}
c <- cor(select(qswurus20, starts_with("rk_")), method = "kendall")
corrplot::corrplot(c)
```



```{r multidimensional scaling of variables}
c.eigen <- eigen(c)
c.coord <- c.eigen$vectors %*% diag(sqrt(c.eigen$values))
plot(c.coord, pch = NA, asp = 1)
arrows(0, 0, c.coord[, 1], c.coord[, 2])
text(c.coord, labels = rownames(c))
```



```{r}
library(ordr)
eigen_ord(c) %>%
  as_tbl_ord() %>%
  augment() %>%
  mutate_v(metric = stringr::str_remove(.name, "rk_")) %>%
  confer_inertia(.5) %>%
  ggbiplot() +
  geom_unit_circle(linetype = "dashed") +
  geom_v_vector() +
  geom_v_text_radiate(aes(label = metric)) +
  scale_x_continuous(expand = expand_scale(add = .4)) +
  scale_y_continuous(expand = expand_scale(add = .2)) +
  ggtitle("MDS of Kendall correlations between university rankings")
```
