---
draft: true
title: the dimensions of (abstracted) polyamory literature
author: ~
date: 
slug: goodreads-polyamory
categories: [curiosity]
tags: [books,polyamory,web scraping,principal components analysis,biplot]
---

```{r knitr options, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  fig.height = 3, fig.width = 8
)
```

I've gained immensely from reading the handful of non-fiction books on polyamory i've made time to, including Dossie Easton and Janet Hardy's _The Ethical Slut_ and Franklin Veaux and Eve Rickert's _More Than Two_. While i've identified as poly since i discovered the term in grad school, i exhibit at least my share of emotional immaturity, and in addition to actual experience building healthy relationships i know i'd benefit from funneling a bit more of this literature into my reading queue. So, with a small group of friends (which has reduced for the time being to someone i'm dating plus myself), i recently started a poly/kink book club.

From the start, i intended to slide fiction, fantasy/scifi, memoir, anthropology, history, and any other genres i could discover onto our shelf, to accrue a well-rounded appreciation for what was available. Though this of course put me to wondering how i could even learn the contours of the poly literature, in order to ensure that i sampled widely from it! Fortunes of timing provided me with three excellent resources:

* [a Goodreads account](https://www.goodreads.com/user/show/57466005);
* [a blog tutorial](https://maraaverick.rbind.io/2017/08/goodreads-part-i-rgoodreads/) by Mara Averick on using R packages to scrape and crunch Goodreads data; and
* [a book](https://www.goodreads.com/list/tag/polyamory) by Julia Silge and David Robinson on doing text mining in tidyverse style.

The post and [its sequel](https://maraaverick.rbind.io/2017/10/goodreads-part-2/) will get you up to speed; i'll outline my web-scraping script and focus mostly on the analysis.

## scrape

It would be, let's say, impractical to manually search out books with explicitly poly content or themes, and even then i'd likely miss a bunch whose descriptions don't let on too clearly. Fortunately, Goodreads allows users both to curate thematic lists _and_ to tag their lists with keywords! For example, [here is the collection of lists tagged "polyamory"](https://www.goodreads.com/list/tag/polyamory), numbering in the dozens. Helpfully, the lists span genres, including fiction, young adult, memoirs, specific configurations like triads, and space opera (natch). Less helpfully, they also range more broadly, for example exotica, sex positivity, and love. On the whole, though, the tag seems like a good candidate for a one-off analysis.

There are other relevant tags, of course, like ["open-relationships"](https://www.goodreads.com/list/tag/open-relationships). But the others i've found turn out to be far less sensitive and often less specific. For example, ["nonmonogamy"](https://www.goodreads.com/list/tag/nonmonogamy) turns up two relevant lists but ["non monogamy"](https://www.goodreads.com/list/tag/non-monogamy) turns up a third, and ["swinging"](https://www.goodreads.com/list/tag/swinging) yields four lists, of which one is a conspicuous false positive. For simplicity, i'll stick with the "polyamory" tag.

My code (in [the supplementary folder](../supplementary/)) scrapes first the book URLs from each list page (`polyamory_list`), then metadata from each book page (`polyamory_books`): title, authors, genres, and the book description. While the authors and title serve as identifiers and the genres as annotations, the descriptions constitute the raw material on which i'll perform a text analysis. The full texts of the books themselves are not so freely available,[^termfreq] the titles are unlikely to reliably encode recurring features, and the genres are likely too few to discriminate except between broad categories.

[^termfreq]: Though making word frequency data publicly available would presumably be straightforward to do and have if anything a positive impact on sales.

## crunch

```{r}
library(tidyverse)
library(tidytext)
library(logisticPCA)
library(ordr)
```

```{r}
polyamory_booklist <-
  read_rds(here::here("supplementary/goodreads-polyamory-booklist.rds"))
```



Copied from original file:

```{r, eval=FALSE}
# use common words in descriptions to coordinate books

polyamory_booklist <- polyamory_booklist %>%
  ungroup() %>%
  select(title = title_page, lists, genres, description)

# word counts within books
polybooks_wordcounts <- polyamory_booklist %>%
  mutate(clean_descr = str_replace_all(description, "[^[:alpha:]\\s]", " ")) %>%
  mutate(clean_descr = str_trim(clean_descr)) %>%
  select(-description) %>%
  unnest_tokens(word, clean_descr) %>%
  distinct() %>%
  anti_join(stop_words, by = "word") %>%
  add_count(title, word, sort = TRUE)

# removal of stopwords

# word usage across books
polybooks_wordusages <- polybooks_wordcounts %>%
  left_join(
    polybooks_wordcounts %>%
      select(title, word, n) %>%
      distinct() %>%
      add_count(word, name = "nn"),
    by = c("title", "word", "n")
  ) %>%
  filter(nn >= 12) %>%
  arrange(desc(nn), desc(n))
# number of words
polybooks_wordusages %>%
  select(word) %>%
  distinct() %>%
  nrow()

# cosine similarity based on common words
polybooks_worddata <- polybooks_wordusages %>%
  #mutate(value = log(n+1)) %>%
  mutate(value = as.integer(n > 0)) %>%
  select(title, lists, genres, word, value) %>%
  mutate(word = paste0(word, "_")) %>%
  spread(word, value, fill = 0)

# logistic singular value decomposition
polybooks_lsvd <- polybooks_worddata %>%
  select(-title, -lists, -genres) %>%
  as.matrix() %>%
  logisticSVD(k = 2) %>%
  as_tbl_ord() %>%
  bind_cols_u(select(polybooks_worddata, title, lists, genres)) %>%
  mutate_u(short_title = str_replace(title, "(: .+$)|( \\(.+$)", "")) %>%
  bind_cols_v(word = names(select(polybooks_worddata, -(1:3)))) %>%
  mutate_v(word = gsub("\\_$", "", word))

polybooks_lsvd_biplot_u <- polybooks_lsvd %>%
  fortify(include = "all") %>%
  slice(chull(select(., 1:2))) %>%
  ggbiplot(aes(x = SC1, y = SC2)) +
  theme_bw() +
  geom_u_point(data = polybooks_lsvd, alpha = .5) +
  geom_u_point(color = "red") +
  geom_u_label_repel(aes(label = short_title), color = "red", alpha = .75)
print(polybooks_lsvd_biplot_u)
polybooks_lsvd_biplot_v <- polybooks_lsvd %>%
  fortify(include = "all") %>%
  filter(.matrix == "v") %>%
  mutate_at(vars(starts_with("SC")), funs(. * 360)) %>%
  slice(chull(select(., 1:2))) %>%
  ggbiplot(aes(x = SC1, y = SC2)) +
  theme_bw() +
  geom_u_point(data = polybooks_lsvd, alpha = .5) +
  geom_v_vector(color = "blue") +
  geom_v_label_repel(aes(label = word), color = "blue", alpha = .75)
print(polybooks_lsvd_biplot_v)

top_u <- polybooks_lsvd %>%
  #get_u() %>%
  fortify(include = "all") %>%
  filter(.matrix == "u") %>%
  slice(chull(select(., 1:2)))
top_v <- polybooks_lsvd %>%
  #get_v() %>%
  fortify(include = "all") %>%
  filter(.matrix == "v") %>%
  mutate_at(vars(starts_with("SC")), funs(. * 360)) %>%
  slice(chull(select(., 1:2)))
polybooks_lsvd_biplot_uv <- polybooks_lsvd %>%
  ggbiplot(aes(x = SC1, y = SC2)) +
  theme_bw() +
  geom_u_point(alpha = .5) +
  geom_v_vector(data = top_v, color = "blue") +
  geom_v_label_repel(data = top_v,
                     aes(label = word), color = "blue", alpha = .75) +
  geom_u_point(data = top_u, color = "red") +
  geom_u_label_repel(data = top_u,
                     aes(label = short_title), color = "red", alpha = .75)
print(polybooks_lsvd_biplot_uv)
```

```{r, eval=FALSE}
# latent Dirichlet allocation

# cluster sample
# https://github.com/tidyverse/dplyr/issues/361#issuecomment-284305359
cluster_sample_n <- function(
  tbl, size, replace = FALSE, weight = NULL, average = FALSE
) {
  weight <- rlang::eval_tidy(enquo(weight), tbl)
  wt_fun <- if (average) mean else sum
  grps <- unlist(lapply(groups(tbl), as.character))
  summ <- summarize(tbl, .weight = if (is.null(weight)) 1 else wt_fun(weight))
  keep <- sample_n(summ, size = size, replace = replace, weight = .weight)
  group_by_at(left_join(select(keep, -.weight), tbl, by = grps), .vars = grps)
}
cluster_sample_frac <- function(
  tbl, size, replace = FALSE, weight = NULL, average = FALSE
) {
  weight <- rlang::eval_tidy(enquo(weight), tbl)
  wt_fun <- if (average) mean else sum
  grps <- unlist(lapply(groups(tbl), as.character))
  summ <- summarize(tbl, .weight = if (is.null(weight)) 1 else wt_fun(weight))
  keep <- sample_frac(summ, size = size, replace = replace, weight = .weight)
  group_by_at(left_join(select(keep, -.weight), tbl, by = grps), .vars = grps)
}

library(topicmodels)
polybooks_lda <- polybooks_wordcounts %>%
  cast_dtm(title, word, n) %>%
  LDA(k = 3) %>%
  tidy()
polybooks_lda %>%
  group_by(term) %>%
  cluster_sample_n(4)

polybooks_top <- polybooks_lda %>%
  group_by(topic) %>%
  top_n(6, beta) %>%
  ungroup() %>%
  mutate(term = reorder(term, beta))
polybooks_top %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# k = 2
polybooks_lda %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > 1e-6 | topic2 > 1e-6) %>%
  mutate(log_ratio = log(topic1 / topic2)) %>%
  top_n(12, abs(log_ratio)) %>%
  mutate(term = fct_reorder(term, log_ratio)) %>%
  ggplot(aes(x = term, y = log_ratio)) +
  geom_col() +
  coord_flip()
# k = 3
polybooks_lda %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > 1e-6 | topic2 > 1e-6 | topic3 > 1e-6) %>%
  mutate(
    topic12 = log(topic1 / topic2),
    topic13 = log(topic1 / topic3),
    topic23 = log(topic2 / topic3)
  ) %>%
  select(-matches("topic[0-9]$")) %>%
  gather(key = "topic_pair", value = "log_ratio", topic12:topic23) %>%
  group_by(topic_pair) %>%
  #top_n(12, abs(log_ratio)) %>%
  {bind_rows(top_n(., 6, log_ratio), top_n(., 6, -log_ratio))} %>%
  ungroup() %>%
  ggplot(aes(x = term, y = log_ratio)) +
  facet_wrap(~ topic_pair, scales = "free_y") +
  geom_col() + coord_flip()
```

```{r, eval=FALSE}
# tf-idf

# tf-idf
polybooks_tfidf <- polybooks_wordcounts %>%
  bind_tf_idf(word, title, n) %>%
  group_by(word) %>% filter(max(tf_idf) > .1) %>% ungroup() %>%
  select(title, lists, genres, word, tf_idf)
polybooks_worddata <- polybooks_tfidf %>%
  mutate(word = paste0(word, "_")) %>%
  spread(word, tf_idf, fill = 0)

# principal components analysis
polybooks_pca <- polybooks_worddata %>%
  select(-title, -lists, -genres) %>%
  as.matrix() %>%
  prcomp() %>%
  as_tbl_ord() %>%
  bind_cols_u(select(polybooks_worddata, title, lists, genres)) %>%
  mutate_u(short_title = str_replace(title, "(: .+$)|( \\(.+$)", "")) %>%
  bind_cols_v(word = names(select(polybooks_worddata, -(1:3)))) %>%
  mutate_v(word = gsub("\\_$", "", word))

polybooks_pca_biplot_u <- polybooks_pca %>%
  fortify(include = "all") %>%
  slice(chull(select(., 1:2))) %>%
  ggbiplot(aes(x = PC1, y = PC2)) +
  theme_bw() +
  geom_u_point(data = polybooks_pca, alpha = .5) +
  geom_u_point(color = "red") +
  geom_u_label_repel(aes(label = short_title), color = "red", alpha = .75)
print(polybooks_pca_biplot_u)
polybooks_pca_biplot_v <- polybooks_pca %>%
  fortify(include = "all") %>%
  filter(.matrix == "v") %>%
  mutate_at(vars(starts_with("SC")), funs(. * 360)) %>%
  slice(chull(select(., 1:2))) %>%
  ggbiplot(aes(x = PC1, y = PC2)) +
  theme_bw() +
  geom_u_point(data = polybooks_pca, alpha = .5) +
  geom_v_vector(color = "blue") +
  geom_v_label_repel(aes(label = word), color = "blue", alpha = .75)
print(polybooks_pca_biplot_v)
```
